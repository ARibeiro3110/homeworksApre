{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework III - Aprendizagem\n",
    "\n",
    "**Afonso da Conceição Ribeiro**, 102763\n",
    "<br>\n",
    "**Miguel Gomes Marques Pessanha de Almeida**, 103493"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Pen-and-paper [12v]\n",
    "\n",
    "**For questions in this group, show your numerical results with 5 decimals or scientific notation. <br>\n",
    "Hint: we highly recommend the use of `numpy` (e.g., `linalg.pinv` for inverse) or other programmatic facilities to support the calculus involved in both questions (1) and (2).**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)\n",
    "\n",
    "**Consider the problem of learning a regression model from 4 bivariate observations**\n",
    "$\n",
    "\\left\\{\n",
    "\\begin{bmatrix} 0.7 \\\\ -0.3 \\end{bmatrix},\n",
    "\\begin{bmatrix} 0.4 \\\\ 0.5 \\end{bmatrix},\n",
    "\\begin{bmatrix} -0.2 \\\\ 0.8 \\end{bmatrix},\n",
    "\\begin{bmatrix} -0.4 \\\\ 0.3 \\end{bmatrix}\n",
    "\\right\\}\n",
    "$\n",
    "**with targets**\n",
    "$\\left( 0.8,0.6,0.3,0.3 \\right)$\n",
    "**.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. [4v]\n",
    "\n",
    "**Given the radial basis function,**\n",
    "$\n",
    "\\phi_j(x) = exp \\left( -\\frac{\\left\\| \\textbf{x}-\\textbf{c}_j \\right\\|^2}{2} \\right)\n",
    "$\n",
    "**, that transforms the original space onto a new space characterized by the similarity of the original observations to the following data points,**\n",
    "$\n",
    "\\left\\{\n",
    "\\textbf{c}_1 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\n",
    "\\textbf{c}_2 = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix},\n",
    "\\textbf{c}_3 = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}\n",
    "\\right\\}\n",
    "$\n",
    "**. <br> Learn the Ridge regression ($l_2$ regularization) using the closed solution with $λ = 0.1$.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [ 0.33914267  0.19945264  0.40096085 -0.29599936]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([(0.7, -0.3), (0.4, 0.5), (-0.2, 0.8), (-0.4, 0.3)])\n",
    "c = np.array([(0, 0), (1, -1), (-1, 1)])\n",
    "z = np.array([0.8, 0.6, 0.3, 0.3])\n",
    "λ = 0.1\n",
    "\n",
    "def phi(x, c):\n",
    "    return np.exp(-(np.linalg.norm(x - c) ** 2) / 2)\n",
    "\n",
    "# Initialize an empty matrix Phi with the same number of rows as x and the same number of columns as c\n",
    "num_x, num_c = x.shape[0], c.shape[0]\n",
    "Phi = np.zeros((x.shape[0], c.shape[0]))\n",
    "\n",
    "# Calculate the values of Phi using the given formula\n",
    "for i in range(num_x):\n",
    "    for j in range(num_c):\n",
    "        Phi[i, j] = phi(x[i], c[j])\n",
    "\n",
    "# Add biases\n",
    "Phi = np.hstack((np.ones((num_x, 1)), Phi))\n",
    "\n",
    "# Calculate w\n",
    "w = np.linalg.inv(Phi.T @ Phi + λ * np.eye(Phi.shape[1])) @ Phi.T @ z\n",
    "\n",
    "print(\"w =\", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textbf{x} = \\begin{bmatrix} 0.7 & -0.3 \\\\ 0.4 & 0.5 \\\\ -0.2 & 0.8 \\\\ -0.4 & 0.3 \\end{bmatrix}, \\quad\n",
    "\\textbf{c} = \\begin{bmatrix} 0 & 0 \\\\ 1 & -1 \\\\ -1 & 1 \\end{bmatrix}, \\quad\n",
    "\\textbf{z} = \\begin{bmatrix} 0.8 \\\\ 0.6 \\\\ 0.3 \\\\ 0.3 \\end{bmatrix}, \\quad\n",
    "\\lambda = 0.1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\phi_j(\\textbf{x}_i) = \\exp\\left(-\\frac{\\|x_i - c_j\\|^2}{2}\\right) \\qquad \\phi_0(\\textbf{x}_i) = 1,  \\forall{i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Phi_{i,j} = \\phi_{j-1}(\\textbf{x}_i) \\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Phi =\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1 & \\phi_1(\\textbf{x}_1) & \\phi_2(\\textbf{x}_1) & \\phi_3(\\textbf{x}_1) \\\\\n",
    "\t\t1 & \\phi_1(\\textbf{x}_2) & \\phi_2(\\textbf{x}_2) & \\phi_3(\\textbf{x}_2) \\\\\n",
    "\t\t1 & \\phi_1(\\textbf{x}_3) & \\phi_2(\\textbf{x}_3) & \\phi_3(\\textbf{x}_3) \\\\\n",
    "\t\t1 & \\phi_1(\\textbf{x}_4) & \\phi_2(\\textbf{x}_4) & \\phi_3(\\textbf{x}_4) \\\\\n",
    "\t\\end{bmatrix}\n",
    "\t=\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1 & 0.74826357 & 0.74826357 & 0.10126646 \\\\\n",
    "\t\t1 & 0.81464732 & 0.27117254 & 0.33121088 \\\\\n",
    "\t\t1 & 0.71177032 & 0.09632764 & 0.71177032 \\\\\n",
    "\t\t1 & 0.88249690 & 0.16121764 & 0.65376979\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Phi^T =\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1          & 1          & 1          & 1          \\\\\n",
    "\t\t0.74826357 & 0.81464732 & 0.71177032 & 0.88249690 \\\\\n",
    "\t\t0.74826357 & 0.27117254 & 0.09632764 & 0.16121764 \\\\\n",
    "\t\t0.10126646 & 0.33121088 & 0.71177032 & 0.65376979\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "(\\Phi^T \\Phi + \\lambda \\textbf{I})^{-1} \\Phi^T =\n",
    "\t\\begin{bmatrix}\n",
    "\t\t 0.14104789 &  0.35022196 &  0.35575370 & -0.30184975 \\\\\n",
    "\t\t-0.09064104 &  0.43822869 & -0.50360629 &  0.53370047 \\\\\n",
    "\t\t 0.99394091 & -0.50614900 & -0.13690469 & -0.16477025 \\\\\n",
    "\t\t-0.31221638 & -0.65245932 &  0.72647200 &  0.42435912\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{w} =\n",
    "\t(\\Phi^T \\Phi + \\lambda I)^{-1} \\Phi^T \\textbf{z}\n",
    "\t=\n",
    "\t\\begin{bmatrix}\n",
    "\t\t 0.14104789 &  0.35022196 &  0.3557537  & -0.30184975 \\\\\n",
    "\t\t-0.09064104 &  0.43822869 & -0.50360629 &  0.53370047 \\\\\n",
    "\t\t 0.99394091 & -0.50614900 & -0.13690469 & -0.16477025 \\\\\n",
    "\t\t-0.31221638 & -0.65245932 &  0.72647200 &  0.42435912\n",
    "\t\\end{bmatrix}\n",
    "\t\\begin{bmatrix}\n",
    "\t\t0.8 \\\\ 0.6 \\\\ 0.3 \\\\ 0.3\n",
    "\t\\end{bmatrix}\n",
    "\t=\n",
    "\t\\begin{bmatrix}\n",
    "\t0.33914267 \\\\ 0.19945264 \\\\ 0.40096085 \\\\ -0.29599936\n",
    "\t\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. [2v]\n",
    "\n",
    "**Compute the training RMSE for the learnt regression.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 0.06508238153393466\n"
     ]
    }
   ],
   "source": [
    "z_hat = Phi @ w\n",
    "\n",
    "RMSE = np.sqrt(np.sum((z - z_hat) ** 2) / num_x)\n",
    "\n",
    "print(\"RMSE =\", RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textbf{z} = \\begin{bmatrix} 0.8 \\\\ 0.6 \\\\ 0.3 \\\\ 0.3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\textbf{z}} =\n",
    "\t\\Phi \\textbf{w}\n",
    "\t=\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1 & 0.74826357 & 0.74826357 & 0.10126646 \\\\\n",
    "\t\t1 & 0.81464732 & 0.27117254 & 0.33121088 \\\\\n",
    "\t\t1 & 0.71177032 & 0.09632764 & 0.71177032 \\\\\n",
    "\t\t1 & 0.88249690  & 0.16121764 & 0.65376979\n",
    "\t\\end{bmatrix}\n",
    "\t\\begin{bmatrix}\n",
    "\t\t0.33914267 \\\\ 0.19945264 \\\\ 0.40096085 \\\\ -0.29599936\n",
    "\t\\end{bmatrix}\n",
    "\t=\n",
    "\t\\begin{bmatrix}\n",
    "\t\t0.75843541 \\\\ 0.51231759 \\\\ 0.30904720 \\\\ 0.38628554\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{RMSE}(\\hat{\\textbf{z}}, \\textbf{z})\n",
    "=\n",
    "\\sqrt{\\frac{1}{4} \\sum_{i=1}^{4} (\\textbf{z}_i - \\hat{\\textbf{z}}_i)^2}\n",
    "=\n",
    "0.06508238\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) [6v]\n",
    "\n",
    "**Consider a MLP classifier of three outcomes - A, B and C - characterized by the weights, <br>**\n",
    "\n",
    "$\n",
    "W^{[1]} =\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 & 1 \\\\\n",
    "1 & 1 & 2 & 1 \\\\\n",
    "1 & 1 & 1 & 1\n",
    "\\end{bmatrix},\n",
    "$\n",
    "$\n",
    "b^{[1]} =\n",
    "\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix},\n",
    "$\n",
    "$\n",
    "W^{[2]} =\n",
    "\\begin{bmatrix}\n",
    "1 & 4 & 1 \\\\\n",
    "1 & 1 & 1\n",
    "\\end{bmatrix},\n",
    "$\n",
    "$\n",
    "b^{[2]} =\n",
    "\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix},\n",
    "$\n",
    "$\n",
    "W^{[3]} =\n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "3 & 1 \\\\\n",
    "1 & 1\n",
    "\\end{bmatrix},\n",
    "$\n",
    "$\n",
    "b^{[3]} =\n",
    "\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\n",
    "$\n",
    "\n",
    "**<br> the activation**\n",
    "$\n",
    "f(x) = \\frac{e^{0.5x-2} - e^{-0.5x+2}}{e^{0.5x-2} + e^{-0.5x+2}} = tanh(0.5x - 2)\n",
    "$\n",
    "**for every unit, and squared error loss**\n",
    "$\n",
    "\\frac{1}{2}\\left\\| \\textbf{z} - \\hat{\\textbf{z}} \\right\\|^2_2\n",
    "$\n",
    "**. Perform one batch gradient descent update (with learning rate $η=0.1$) for training observations**\n",
    "$\n",
    "\\textbf{x}_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\n",
    "$\n",
    "**and**\n",
    "$\n",
    "\\textbf{x}_2 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ -1 \\end{bmatrix}\n",
    "$\n",
    "**with targets B and A, respectively.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[1.0187207  1.01871904 1.01871904 1.01871737]\n",
      " [1.03358917 1.03358719 2.03358719 1.03358521]\n",
      " [1.0187207  1.01871904 1.01871904 1.01871737]]\n",
      "\n",
      "b1 = [[1.0187207 ]\n",
      " [1.03358917]\n",
      " [1.0187207 ]]\n",
      "\n",
      "W2 = [[1.01730444 4.02851932 1.01730444]\n",
      " [1.00467751 1.00771893 1.00467751]]\n",
      "\n",
      "b2 = [[1.0374494 ]\n",
      " [1.01017306]]\n",
      "\n",
      "W3 = [[0.99703633 0.9977484 ]\n",
      " [3.01431372 0.98168546]\n",
      " [0.99971282 1.00040847]]\n",
      "\n",
      "b3 = [[1.00198208]\n",
      " [1.03177312]\n",
      " [0.99930442]]\n"
     ]
    }
   ],
   "source": [
    "# Weights and biases\n",
    "W1 = np.array([[1, 1, 1, 1], [1, 1, 2, 1], [1, 1, 1, 1]])\n",
    "b1 = np.array([[1], [1], [1]])\n",
    "W2 = np.array([[1, 4, 1], [1, 1, 1]])\n",
    "b2 = np.array([[1], [1]])\n",
    "W3 = np.array([[1, 1], [3, 1], [1, 1]])\n",
    "b3 = np.array([[1], [1], [1]])\n",
    "\n",
    "def tanh(x):\n",
    "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "# Activation function\n",
    "def f(x):\n",
    "    return tanh(0.5*x - 2)\n",
    "\n",
    "def f_(x):\n",
    "    return (1 - f(x)**2) * 0.5\n",
    "\n",
    "# Loss function\n",
    "def E(x, t):\n",
    "    return 0.5 * np.sum(np.linalg.norm(t - x) ** 2)\n",
    "\n",
    "# Learning rate\n",
    "η = 0.1\n",
    "\n",
    "# Training observations and targets\n",
    "x0_1 = np.array([[1], [1], [1], [1]])\n",
    "x0_2 = np.array([[1], [0], [0], [-1]])\n",
    "t_1 = np.array([[-1], [1], [-1]])\n",
    "t_2 = np.array([[1], [-1], [-1]])\n",
    "\n",
    "# Forward propagation\n",
    "z1_1 = W1 @ x0_1 + b1\n",
    "x1_1 = f(z1_1)\n",
    "z2_1 = W2 @ x1_1 + b2\n",
    "x2_1 = f(z2_1)\n",
    "z3_1 = W3 @ x2_1 + b3\n",
    "x3_1 = f(z3_1)\n",
    "z1_2 = W1 @ x0_2 + b1\n",
    "x1_2 = f(z1_2)\n",
    "z2_2 = W2 @ x1_2 + b2\n",
    "x2_2 = f(z2_2)\n",
    "z3_2 = W3 @ x2_2 + b3\n",
    "x3_2 = f(z3_2)\n",
    "\n",
    "# Backward propagation\n",
    "δ3_1 = (x3_1 - t_1) * f_(z3_1)\n",
    "δ2_1 = W3.T @ δ3_1 * f_(z2_1)\n",
    "δ1_1 = W2.T @ δ2_1 * f_(z1_1)\n",
    "δ3_2 = (x3_2 - t_2) * f_(z3_2)\n",
    "δ2_2 = W3.T @ δ3_2 * f_(z2_2)\n",
    "δ1_2 = W2.T @ δ2_2 * f_(z1_2)\n",
    "\n",
    "# Updates:\n",
    "W1 = W1 - η * (δ1_1 @ x0_1.T + δ1_2 @ x0_2.T)\n",
    "b1 = b1 - η * (δ1_1 + δ1_2)\n",
    "W2 = W2 - η * (δ2_1 @ x1_1.T + δ2_2 @ x1_2.T)\n",
    "b2 = b2 - η * (δ2_1 + δ2_2)\n",
    "W3 = W3 - η * (δ3_1 @ x2_1.T + δ3_2 @ x2_2.T)\n",
    "b3 = b3 - η * (δ3_1 + δ3_2)\n",
    "\n",
    "print(\"W1 =\", W1)\n",
    "print(\"\\nb1 =\", b1)\n",
    "print(\"\\nW2 =\", W2)\n",
    "print(\"\\nb2 =\", b2)\n",
    "print(\"\\nW3 =\", W3)\n",
    "print(\"\\nb3 =\", b3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forward Propagation\n",
    "\n",
    "$$\n",
    "\\textbf{z}_k^{[i]} = \\textbf{W}^{[i]} \\; \\textbf{x}^{[i-1]} + \\textbf{b}^{[i]} \\qquad\n",
    "\\textbf{x}_k^{[i]} = tanh(0.5 \\; \\textbf{z}_k^{[i]} - 2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{z}_1^{[1]} =\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1 & 1 & 1 & 1 \\\\\n",
    "\t\t1 & 1 & 2 & 1 \\\\\n",
    "\t\t1 & 1 & 1 & 1\n",
    "\t\\end{bmatrix}\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1 \\\\ 1 \\\\ 1 \\\\ 1\n",
    "\t\\end{bmatrix}\n",
    "\t+\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1 \\\\ 1 \\\\ 1\n",
    "\t\\end{bmatrix}\n",
    "\t=\n",
    "\t\\begin{bmatrix}\n",
    "\t\t5 \\\\ 6 \\\\ 5\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{x}_1^{[1]} =\n",
    "\ttanh \\left(\n",
    "\t\\begin{bmatrix}\n",
    "\t\t0.5 \\cdot 5 - 2 \\\\\n",
    "\t\t0.5 \\cdot 6 - 2 \\\\\n",
    "\t\t0.5 \\cdot 5 - 2\n",
    "\t\\end{bmatrix}\n",
    "\t\\right)\n",
    "\t=\n",
    "\t\\begin{bmatrix}\n",
    "\t\t0.46211716 \\\\ 0.76159416 \\\\ 0.46211716\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{z}_1^{[2]} =\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1 & 4 & 1 \\\\\n",
    "\t\t1 & 1 & 1\n",
    "\t\\end{bmatrix}\n",
    "\t\\begin{bmatrix}\n",
    "\t\t0.46211716 \\\\ 0.76159416 \\\\ 0.46211716\n",
    "\t\\end{bmatrix}\n",
    "\t+\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1 \\\\ 1\n",
    "\t\\end{bmatrix}\n",
    "\t=\n",
    "\t\\begin{bmatrix}\n",
    "\t\t4.97061094 \\\\ 2.68582847\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{x}_1^{[2]} =\n",
    "\ttanh \\left(\n",
    "\t\\begin{bmatrix}\n",
    "\t\t0.5 \\cdot 4.97061094 - 2 \\\\\n",
    "\t\t0.5 \\cdot 2.68582847 - 2\n",
    "\t\\end{bmatrix}\n",
    "\t\\right)\n",
    "\t=\n",
    "\t\\begin{bmatrix}\n",
    "\t\t0.45048251 \\\\ -0.57642073\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{z}_1^{[3]} =\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1 & 1 \\\\\n",
    "\t\t3 & 1 \\\\\n",
    "\t\t1 & 1\n",
    "\t\\end{bmatrix}\n",
    "\t\\begin{bmatrix}\n",
    "\t\t0.45048251 \\\\ -0.57642073\n",
    "\t\\end{bmatrix}\n",
    "\t+\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1 \\\\ 1 \\\\ 1\n",
    "\t\\end{bmatrix}\n",
    "\t=\n",
    "\t\\begin{bmatrix}\n",
    "\t\t0.87406178 \\\\ 1.77502679 \\\\ 0.87406178\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{x}_1^{[3]} =\n",
    "\ttanh \\left(\n",
    "\t\\begin{bmatrix}\n",
    "\t\t0.5 \\cdot 0.87406178 - 2 \\\\\n",
    "\t\t0.5 \\cdot 1.77502679 - 2 \\\\\n",
    "\t\t0.5 \\cdot 0.87406178 - 2\n",
    "\t\\end{bmatrix}\n",
    "\t\\right)\n",
    "\t=\n",
    "\t\\begin{bmatrix}\n",
    "\t\t-0.91590016 \\\\ -0.80493961 \\\\ -0.91590016\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{z}_2^{[1]} =\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1 & 1 & 1 & 1 \\\\\n",
    "\t\t1 & 1 & 2 & 1 \\\\\n",
    "\t\t1 & 1 & 1 & 1\n",
    "\t\\end{bmatrix}\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1 \\\\ 0 \\\\ 0 \\\\ -1\n",
    "\t\\end{bmatrix}\n",
    "\t+\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1 \\\\ 1 \\\\ 1\n",
    "\t\\end{bmatrix}\n",
    "\t=\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1 \\\\ 1 \\\\ 1\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{x}_2^{[1]} =\n",
    "\ttanh \\left(\n",
    "\t\\begin{bmatrix}\n",
    "\t\t0.5 \\cdot 1 - 2 \\\\\n",
    "\t\t0.5 \\cdot 1 - 2 \\\\\n",
    "\t\t0.5 \\cdot 1 - 2\n",
    "\t\\end{bmatrix}\n",
    "\t\\right)\n",
    "\t=\n",
    "\t\\begin{bmatrix}\n",
    "\t\t-0.90514825 \\\\ -0.90514825 \\\\ -0.90514825\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{z}_2^{[2]} =\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1 & 4 & 1 \\\\\n",
    "\t\t1 & 1 & 1\n",
    "\t\\end{bmatrix}\n",
    "\t\\begin{bmatrix}\n",
    "\t\t-0.90514825 \\\\ -0.90514825 \\\\ -0.90514825\n",
    "\t\\end{bmatrix}\n",
    "\t+\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1 \\\\ 1\n",
    "\t\\end{bmatrix}\n",
    "\t=\n",
    "\t\\begin{bmatrix}\n",
    "\t\t-4.43088952 \\\\ -1.71544476\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{x}_2^{[2]} =\n",
    "\ttanh \\left(\n",
    "\t\\begin{bmatrix}\n",
    "\t\t0.5 \\cdot -4.43088952 - 2 \\\\\n",
    "\t\t0.5 \\cdot -1.71544476 - 2\n",
    "\t\\end{bmatrix}\n",
    "\t\\right)\n",
    "\t=\n",
    "\t\\begin{bmatrix}\n",
    "\t\t-0.99956404 \\\\ -0.99343227\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{z}_2^{[3]} =\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1 & 1 \\\\\n",
    "\t\t3 & 1 \\\\\n",
    "\t\t1 & 1\n",
    "\t\\end{bmatrix}\n",
    "\t\\begin{bmatrix}\n",
    "\t\t-0.99956404 \\\\ -0.99343227\n",
    "\t\\end{bmatrix}\n",
    "\t+\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1 \\\\ 1 \\\\ 1\n",
    "\t\\end{bmatrix}\n",
    "\t=\n",
    "\t\\begin{bmatrix}\n",
    "\t\t-0.99299631 \\\\ -2.99212439 \\\\ -0.99299631\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{x}_2^{[3]} =\n",
    "\ttanh \\left(\n",
    "\t\\begin{bmatrix}\n",
    "\t\t0.5 \\cdot -0.99299631 - 2 \\\\\n",
    "\t\t0.5 \\cdot -2.99212439 - 2 \\\\\n",
    "\t\t0.5 \\cdot -0.99299631 - 2\n",
    "\t\\end{bmatrix}\n",
    "\t\\right)\n",
    "\t=\n",
    "\t\\begin{bmatrix}\n",
    "\t\t-0.98652085 \\\\ -0.99816350 \\\\ -0.98652085\n",
    "\t\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Backward Propagation\n",
    "\n",
    "$$\n",
    "\\textbf{t}_1 = \\begin{bmatrix} -1 \\\\ 1 \\\\ -1 \\end{bmatrix}\n",
    "\\textbf{t}_2 = \\begin{bmatrix} 1 \\\\ -1 \\\\ -1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "E(\\textbf{x}_k^{[3]}, \\textbf{t}_i)\n",
    "=\n",
    "\\frac{1}{2} \\sum_{k=1}^{2} (\\textbf{t}_{k} - \\hat{\\textbf{z}_k})^2\n",
    "=\n",
    "\\frac{1}{2} \\sum_{k=1}^{2} (\\textbf{t}_{k} - \\textbf{x}_k^{[3]})^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E(\\textbf{x}_k^{[3]}, \\textbf{t}_i)}{\\partial \\textbf{x}_k^{[i]}}\n",
    "=\n",
    "\\frac{1}{2} \\cdot 2 (\\textbf{x}_k^{[3]} - \\textbf{t}_k)\n",
    "=\n",
    "\\textbf{x}_k^{[3]} - \\textbf{t}_k\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\textbf{x}_k^{[i]}(\\textbf{z}_k^{[i]})}{\\partial \\textbf{z}_k^{[i]}}\n",
    "=\n",
    "\\frac{\\partial \\; tanh(0.5 \\; \\textbf{z}_k^{[i]} - 2)}{\\partial \\textbf{z}_k^{[i]}}\n",
    "=\n",
    "\\frac{1}{2} \\; (1 - tanh^2(0.5 \\; \\textbf{z}_k^{[i]} - 2))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\textbf{z}_k^{[i]}(\\textbf{W}^{[i]}, \\textbf{b}^{[i]}, \\textbf{x}_k^{[i-1]})}{\\partial \\textbf{W}^{[i]}}\n",
    "=\n",
    "\\textbf{x}_k^{[i-1]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\textbf{z}_k^{[i]}(\\textbf{W}^{[i]}, \\textbf{b}^{[i]}, \\textbf{x}_k^{[i-1]})}{\\partial \\textbf{b}^{[i]}}\n",
    "= 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\textbf{z}_k^{[i]}(\\textbf{W}^{[i]}, \\textbf{b}^{[i]}, \\textbf{x}_k^{[i-1]})}{\\partial \\textbf{x}_k^{[i-1]}}\n",
    "=\n",
    "\\textbf{W}^{[i]}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deltas:\n",
    "\n",
    "$$\n",
    "\\delta_k^{[3]}\n",
    "=\n",
    "\\frac{\\partial E}{\\partial \\textbf{x}_k^{[3]}}\n",
    "\\circ\n",
    "\\frac{\\partial \\textbf{x}_k^{[3]}}{\\partial \\textbf{z}_k^{[3]}}\n",
    "=\n",
    "(\\textbf{x}_k^{[3]} - \\textbf{t}_k)\n",
    "\\circ\n",
    "\\frac{1}{2} \\; (1 - tanh^2(0.5 \\; \\textbf{z}_k^{[3]} - 2))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta_k^{[l]}\n",
    "=\n",
    "\\left( \\frac{\\partial \\textbf{z}_k^{[l+1]}}{\\partial \\textbf{x}_k^{[l]}} \\right) ^T\n",
    "\\cdot\n",
    "\\delta_k^{[l+1]}\n",
    "\\circ\n",
    "\\frac{\\partial \\textbf{x}_k^{[l]}}{\\partial \\textbf{z}_k^{[l]}}\n",
    "=\n",
    "{\\textbf{W}^{[l+1]}}^T\n",
    "\\cdot\n",
    "\\delta_k^{[l+1]}\n",
    "\\circ\n",
    "\\frac{1}{2} \\; (1 - tanh^2(0.5 \\; \\textbf{z}_k^{[l]} - 2)), \\quad l \\in \\{1, 2\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta_1^{[3]} =\n",
    "\t\\left(\n",
    "\t\\begin{bmatrix} -0.91590016 \\\\ -0.80493961 \\\\ -0.91590016 \\end{bmatrix}\n",
    "\t-\n",
    "\t\\begin{bmatrix} -1 \\\\ 1 \\\\ -1 \\end{bmatrix}\n",
    "\t\\right)\n",
    "\t\\circ\n",
    "\t\\begin{bmatrix} 0.08056345 \\\\ 0.17603611 \\\\ 0.08056345 \\end{bmatrix}\n",
    "\t=\n",
    "\t\\begin{bmatrix} 0.00677537 \\\\ -0.31773455 \\\\ 0.00677537 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta_1^{[2]} =\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1 & 1 \\\\\n",
    "\t\t3 & 1 \\\\\n",
    "\t\t1 & 1\n",
    "\t\\end{bmatrix}^T\n",
    "\t\\cdot\n",
    "\t\\begin{bmatrix} 0.00677537 \\\\ -0.31773455 \\\\ 0.00677537 \\end{bmatrix}\n",
    "\t\\circ\n",
    "\t\\begin{bmatrix} 0.39853275 \\\\ 0.33386957 \\end{bmatrix}\n",
    "\t=\n",
    "\t\\begin{bmatrix} -0.37448246 \\\\ -0.10155772 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta_1^{[1]} =\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1 & 4 & 1 \\\\\n",
    "\t\t1 & 1 & 1\n",
    "\t\\end{bmatrix}^T\n",
    "\t\\cdot\n",
    "\t\\begin{bmatrix} -0.37448246 \\\\ -0.10155772 \\end{bmatrix}\n",
    "\t\\circ\n",
    "\t\\begin{bmatrix} 0.39322387 \\\\ 0.20998717 \\\\ 0.39322387 \\end{bmatrix}\n",
    "\t=\n",
    "\t\\begin{bmatrix} -0.18719036 \\\\ -0.33587187 \\\\ -0.18719036 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta_2^{[3]} =\n",
    "\t\\left(\n",
    "\t\\begin{bmatrix} -0.98652085 \\\\ -0.99816350 \\\\ -0.98652085 \\end{bmatrix}\n",
    "\t-\n",
    "\t\\begin{bmatrix} 1 \\\\ -1 \\\\ -1 \\end{bmatrix}\n",
    "\t\\right)\n",
    "\t\\circ\n",
    "\t\\begin{bmatrix} 0.01338830 \\\\ 0.00183481 \\\\ 0.01338830 \\end{bmatrix}\n",
    "\t=\n",
    "\t\\begin{bmatrix} -0.02659614 \\\\ 0.00000337 \\\\ 0.00018046 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta_2^{[2]} =\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1 & 1 \\\\\n",
    "\t\t3 & 1 \\\\\n",
    "\t\t1 & 1\n",
    "\t\\end{bmatrix}^T\n",
    "\t\\cdot\n",
    "\t\\begin{bmatrix} -0.02659614 \\\\ 0.00000337 \\\\ 0.00018046 \\end{bmatrix}\n",
    "\t\\circ\n",
    "\t\\begin{bmatrix}\n",
    "\t0.00043586 \\\\ 0.00654616\n",
    "\t\\end{bmatrix}\n",
    "\t=\n",
    "\t\\begin{bmatrix} -0.00001151 \\\\ -0.00017290 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\delta_2^{[1]} =\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1 & 4 & 1 \\\\\n",
    "\t\t1 & 1 & 1\n",
    "\t\\end{bmatrix}^T\n",
    "\t\\cdot\n",
    "\t\\begin{bmatrix} -0.00001151 \\\\ -0.00017290 \\end{bmatrix}\n",
    "\t\\circ\n",
    "\t\\begin{bmatrix}\n",
    "\t0.09035332 \\\\ 0.09035332 \\\\ 0.09035332\n",
    "\t\\end{bmatrix}\n",
    "\t=\n",
    "\t\\begin{bmatrix} -0.00001666 \\\\ -0.00001978 \\\\ -0.00001666 \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Updates:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial \\textbf{W}^{[i]}}\n",
    "=\n",
    "\\delta_1^{[i]}\n",
    "\\frac{\\partial \\textbf{z}_1^{[i]}}{\\partial \\textbf{W}^{[i]}}\n",
    "+\n",
    "\\delta_2^{[i]}\n",
    "\\frac{\\partial \\textbf{z}_2^{[i]}}{\\partial \\textbf{W}^{[i]}}\n",
    "=\n",
    "\\delta_1^{[i]}\n",
    "{\\textbf{x}_1^{[i-1]}}^T\n",
    "+\n",
    "\\delta_2^{[i]}\n",
    "{\\textbf{x}_2^{[i-1]}}^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial \\textbf{b}^{[i]}}\n",
    "=\n",
    "\\delta_1^{[i]}\n",
    "\\frac{\\partial \\textbf{z}_1^{[i]}}{\\partial \\textbf{b}^{[i]}}\n",
    "+\n",
    "\\delta_2^{[i]}\n",
    "\\frac{\\partial \\textbf{z}_2^{[i]}}{\\partial \\textbf{b}^{[i]}}\n",
    "=\n",
    "\\delta_1^{[i]}\n",
    "+\n",
    "\\delta_2^{[i]}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{W}^{[i]} = \\textbf{W}^{[i]} - \\eta \\frac{\\partial E}{\\partial \\textbf{W}^{[i]}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial \\textbf{W}^{[1]}}\n",
    "=\n",
    "\\begin{bmatrix} -0.18719036 \\\\ -0.33587187 \\\\ -0.18719036 \\end{bmatrix}\n",
    "\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}^T\n",
    "+\n",
    "\\begin{bmatrix} -0.00001666 \\\\ -0.00001978 \\\\ -0.00001666 \\end{bmatrix}\n",
    "\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ -1 \\end{bmatrix}^T\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\t-0.18720702 & -0.18719036 & -0.18719036 & -0.18717370 \\\\\n",
    "\t-0.33589165 & -0.33587187 & -0.33587187 & -0.33585209 \\\\\n",
    "\t-0.18720702 & -0.18719036 & -0.18719036 & -0.18717370\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial \\textbf{W}^{[2]}}\n",
    "=\n",
    "\\begin{bmatrix} -0.37448246 \\\\ -0.10155772 \\end{bmatrix}\n",
    "\\begin{bmatrix} 0.46211716 \\\\ 0.76159416 \\\\ 0.46211716 \\end{bmatrix}^T\n",
    "+\n",
    "\\begin{bmatrix} -0.00001151 \\\\ -0.00017290 \\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\t\t-0.90514825 \\\\ -0.90514825 \\\\ -0.90514825\n",
    "\t\\end{bmatrix}^T\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\t-0.17304435 & -0.28519324 & -0.17304435 \\\\\n",
    "\t-0.04677506 & -0.07718926 & -0.04677506\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial \\textbf{W}^{[3]}}\n",
    "=\n",
    "\\begin{bmatrix} 0.00677537 \\\\ -0.31773455 \\\\ 0.00677537 \\end{bmatrix}\n",
    "\\begin{bmatrix} 0.45048251 \\\\ -0.57642073 \\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix} -0.02659614 \\\\ 0.00000337 \\\\ 0.00018046 \\end{bmatrix}\n",
    "\\begin{bmatrix} -0.99956404 \\\\ -0.99343227 \\end{bmatrix}^T\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\t0.02963673 & 0.02251600 \\\\\n",
    "\t-0.14313723 & 0.18314544 \\\\\n",
    "\t0.00287180 & -0.00408474\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial \\textbf{b}^{[1]}}\n",
    "=\n",
    "\\begin{bmatrix} -0.18719036 \\\\ -0.33587187 \\\\ -0.18719036 \\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix} -0.00001666 \\\\ -0.00001978 \\\\ -0.00001666 \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} -0.18720702 \\\\ -0.33589165 \\\\ -0.18720702 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial \\textbf{b}^{[2]}}\n",
    "=\n",
    "\\begin{bmatrix} -0.37448246 \\\\ -0.10155772 \\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix} -0.00001151 \\\\ -0.00017290 \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} -0.37449397 \\\\ -0.10173062 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial \\textbf{b}^{[3]}}\n",
    "=\n",
    "\\begin{bmatrix} 0.00677537 \\\\ -0.31773455 \\\\ 0.00677537 \\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix} -0.02659614 \\\\ 0.00000337 \\\\ 0.00018046 \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} -0.01982077 \\\\ -0.31773118 \\\\ 0.00695584 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{W}^{[1]} =\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1 & 1 & 1 & 1 \\\\\n",
    "\t\t1 & 1 & 2 & 1 \\\\\n",
    "\t\t1 & 1 & 1 & 1\n",
    "\t\\end{bmatrix}\n",
    "\t-\n",
    "\t0.1\n",
    "\t\\begin{bmatrix}\n",
    "\t\t-0.18720702 & -0.18719036 & -0.18719036 & -0.18717370 \\\\\n",
    "\t\t-0.33589165 & -0.33587187 & -0.33587187 & -0.33585209 \\\\\n",
    "\t\t-0.18720702 & -0.18719036 & -0.18719036 & -0.18717370\n",
    "\t\\end{bmatrix}\n",
    "\t= \\\\ =\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1.01872070 & 1.01871904 & 1.01871904 & 1.01871737 \\\\\n",
    "\t\t1.03358917 & 1.03358719 & 2.03358719 & 1.03358521 \\\\\n",
    "\t\t1.01872070 & 1.01871904 & 1.01871904 & 1.01871737\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{W}^{[2]} =\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1 & 4 & 1 \\\\\n",
    "\t\t1 & 1 & 1\n",
    "\t\\end{bmatrix}\n",
    "\t-\n",
    "\t0.1\n",
    "\t\\begin{bmatrix}\n",
    "\t\t-0.17304435 & -0.28519324 & -0.17304435 \\\\\n",
    "\t\t-0.04677506 & -0.07718926 & -0.04677506\n",
    "\t\\end{bmatrix}\n",
    "\t=\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1.01730444 & 4.02851932 & 1.01730444 \\\\\n",
    "\t\t1.00467751 & 1.00771893 & 1.00467751\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{W}^{[3]} =\n",
    "\t\\begin{bmatrix}\n",
    "\t\t1 & 1 \\\\\n",
    "\t\t3 & 1 \\\\\n",
    "\t\t1 & 1\n",
    "\t\\end{bmatrix}\n",
    "\t-\n",
    "\t0.1\n",
    "\t\\begin{bmatrix}\n",
    "\t\t0.02963673 & 0.022516 \\\\\n",
    "\t\t-0.14313723 & 0.18314544 \\\\\n",
    "\t\t0.0028718 & -0.00408474\n",
    "\t\\end{bmatrix}\n",
    "\t=\n",
    "\t\\begin{bmatrix}\n",
    "\t\t0.99703633 & 0.9977484 \\\\\n",
    "\t\t3.01431372 & 0.98168546 \\\\\n",
    "\t\t0.99971282 & 1.00040847\n",
    "\t\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{b}^{[1]} =\n",
    "\t\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\n",
    "\t-\n",
    "\t0.1\n",
    "\t\\begin{bmatrix} -0.18720702 \\\\ -0.33589165 \\\\ -0.18720702 \\end{bmatrix}\n",
    "\t=\n",
    "\t\\begin{bmatrix} 1.0187207 \\\\ 1.03358917 \\\\ 1.0187207 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{b}^{[2]} =\n",
    "\t\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\n",
    "\t-\n",
    "\t0.1\n",
    "\t\\begin{bmatrix} -0.37449397 \\\\ -0.10173062 \\end{bmatrix}\n",
    "\t=\n",
    "\t\\begin{bmatrix} 1.0374494 \\\\ 1.01017306 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\textbf{b}^{[3]} =\n",
    "\t\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}\n",
    "\t-\n",
    "\t0.1\n",
    "\t\\begin{bmatrix} -0.01982077 \\\\ -0.31773118 \\\\ 0.00695584 \\end{bmatrix}\n",
    "\t=\n",
    "\t\\begin{bmatrix} 1.00198208 \\\\ 1.03177312 \\\\ 0.99930442 \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Programming and critical analysis [8v]\n",
    "\n",
    "**Consider the `winequality-red.csv` dataset (available at the webpage) where the goal is to estimate the quality (sensory appreciation) of a wine based on physicochemical inputs.\n",
    "<br> <br>\n",
    "Using a 80-20 training-test split with a fixed seed (`random_state=0`), you are asked to learn MLP regressors to answer the following questions.\n",
    "<br> <br>\n",
    "Given their stochastic behavior, average the performance of each MLP from 10 runs (for reproducibility consider seeding the MLPs with `random_state ∈ {1..10}`).**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from scipy.io.arff import loadarff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection, metrics\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(\"./winequality-red.csv\", delimiter=\";\")\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Separate target and features\n",
    "X = df.drop(\"quality\", axis=1)\n",
    "y = df[\"quality\"]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) [3.5v]\n",
    "\n",
    "**Learn a MLP regressor with 2 hidden layers of size 10, rectifier linear unit activation on all nodes, and early stopping with 20% of training data set aside for validation. All remaining parameters (e.g., loss, batch size, regularization term, solver) should be set as default. Plot the distribution of the residues (in absolute value) using a histogram.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "abs_errors = []\n",
    "\n",
    "for random_state in range(1, 11): # 10 runs\n",
    "    # Create MLP regressor\n",
    "    mlp = MLPRegressor(\n",
    "        hidden_layer_sizes=(10, 10),\n",
    "        activation='relu',\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.2,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "    # Predict test data\n",
    "    y_pred = mlp.predict(X_test)\n",
    "\n",
    "    # Calculate absolute error\n",
    "    abs_error = abs(y_test - y_pred)\n",
    "    abs_errors.append(abs_error)\n",
    "\n",
    "# Calculate the element-wise average of the absolute errors\n",
    "avg_abs_error = np.mean(abs_errors, axis=0)\n",
    "\n",
    "# Plot the histogram of the average absolute errors\n",
    "plt.hist(avg_abs_error, bins=20)\n",
    "plt.title(\"Absolute Residuals (Averaging Over 10 Runs)\")\n",
    "plt.xlabel(\"Average Absolute Error\")\n",
    "plt.ylabel(\"Absolute Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) [1.5v]\n",
    "\n",
    "**Since we are in the presence of a integer regression task, a recommended trick is to round and bound estimates. Assess the impact of these operations on the MAE of the MLP learnt in previous question.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original MAE:  0.5097171955009514\n",
      "Rounded MAE:  0.43875000000000003\n"
     ]
    }
   ],
   "source": [
    "originalMAE = []\n",
    "roundedMAE = []\n",
    "\n",
    "for random_state in range(1, 11):\n",
    "    \n",
    "    mlp = MLPRegressor(\n",
    "        hidden_layer_sizes=(10, 10),\n",
    "        activation='relu',\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.2,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = mlp.predict(X_test)\n",
    "    \n",
    "    # Round and bound estimates\n",
    "    y_pred_rounded = np.round(y_pred)\n",
    "    y_pred_rounded[y_pred_rounded > 10] = 10\n",
    "    y_pred_rounded[y_pred_rounded < 1] = 1\n",
    "\n",
    "    # Original MAE\n",
    "    originalMAE.append(metrics.mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "    # Rounded MAE\n",
    "    roundedMAE.append(metrics.mean_absolute_error(y_test, y_pred_rounded))\n",
    "\n",
    "print(\"Original MAE: \", np.mean(originalMAE))\n",
    "print(\"Rounded MAE: \", np.mean(roundedMAE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**${\\text{OriginalMAE:} \\; 0.5097}$**\n",
    "\n",
    "**${\\text{RoundedMAE:} \\; 0.4388}$**\n",
    "\n",
    "Indeed, the post-processing of rounding and bounding the model estimates **affects substantially**  the MAE of the learnt MLP regressor. This outcome is likely due to the **better approximation of our predictions to the dataset domain**, as we know beforehand that we are working with an integer regression in the interval ${1 .. 10}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) [1.5v]\n",
    "\n",
    "**Similarly assess the impact on RMSE from replacing early stopping by a well-defined number of iterations in {20,50,100,200} (where one iteration corresponds to a batch).**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for 20 iterations:  1.4039789509925442\n",
      "RMSE for 50 iterations:  0.7996073631460566\n",
      "RMSE for 100 iterations:  0.6940361469112144\n",
      "RMSE for 200 iterations:  0.6554543932216474\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rmse_errors = []\n",
    "for max_iter in [20, 50, 100, 200]:\n",
    "    rmses = np.array([])\n",
    "    for r in range(10):\n",
    "        mlp = MLPRegressor(\n",
    "            hidden_layer_sizes=(10, 10),\n",
    "            activation='relu',\n",
    "            max_iter=max_iter,\n",
    "            random_state=r+1\n",
    "        )\n",
    "\n",
    "        mlp.fit(X_train, y_train)\n",
    "        y_pred = mlp.predict(X_test)\n",
    "        rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "        rmses = np.append(rmses, rmse)\n",
    "    \n",
    "    print(\"RMSE for\", max_iter, \"iterations: \", np.mean(rmses))\n",
    "    rmse_errors.append(rmse)\n",
    "\n",
    "plt.plot([20, 50, 100, 200], rmse_errors)\n",
    "plt.title(\"RMSE Variation\")\n",
    "plt.xlabel(\"Max Iterations\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By increasing the model's maximum number of iterations from 20 to 50, we observe a **signficant drop in the RMSE loss metric** which correlates to a **substancial increase in model performance**. As we further increase this hyperparameter to 100 and 200, we find that the **performace improvements are minimal**, and concerns now focus more on the **bias-variance trade-off**, to prevent the model from overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) [1.5v]\n",
    "\n",
    "**Critically comment the results obtained in previous question, hypothesizing at least one reason why early stopping favors and/or worsens performance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we allow the model to train for more than 100 iterations, the marginal improvements in training accuracy **become less significant compared to the increased risk of model overfitting**. In this context, **the early stopping criterion becomes advantageous**, as it enables us to halt training at the optimal point for **better generalization to test data**. It is crucial to find the right balance between model complexity and training duration. This practice not only enhances generalization and avoids overfitting, but also contributes to the model's efficiency and resource utilization, making it an essential technique in the realm of machine learning and neural networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
